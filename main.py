import urllib.parse
import feedparser
import os
import json
import requests
import google.generativeai as genai
import datetime
import time
import openai
from openai import OpenAI

# --- å®‰å…¨é…ç½®åŒº ---
GEMINI_KEY = os.getenv("GEMINI_KEY")
DEEPSEEK_KEY = os.getenv("DEEPSEEK_KEY")
WECHAT_WEBHOOK = os.getenv("WECHAT_WEBHOOK")
DB_FILE = "read_papers.json"
TOPIC = "(cat:cs.AI OR cat:cs.CV OR cat:cs.LG)"

def send_to_wechat(content):
    if not WECHAT_WEBHOOK:
        print("æœªæ£€æµ‹åˆ° Webhookï¼Œè·³è¿‡æ¨é€")
        return
    headers = {"Content-Type": "application/json"}
    payload = {
        "msgtype": "markdown",
        "markdown": {"content": content}
    }
    try:
        requests.post(WECHAT_WEBHOOK, json=payload, headers=headers)
    except Exception as e:
        print(f"æ¨é€å¾®ä¿¡å¤±è´¥: {e}")

def load_read_papers():
    if os.path.exists(DB_FILE):
        try:
            with open(DB_FILE, 'r') as f:
                return json.load(f)
        except: 
            return [] 
    return []

def save_read_paper(paper_id):
    read_list = load_read_papers()
    if paper_id not in read_list:
        read_list.append(paper_id)
        with open(DB_FILE, 'w') as f:
            json.dump(read_list[-100:], f)

def get_ai_summary(title, summary):
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”é¢†è·¯äººã€‚è¯·é˜…è¯»ä»¥ä¸‹è®ºæ–‡å¹¶è¿›è¡Œç­›é€‰ï¼š
    æ ‡é¢˜ï¼š{title}
    æ‘˜è¦ï¼š{summary}

    ç­›é€‰å‡†åˆ™ï¼š
    1. ä¼˜å…ˆæ€»ç»“å…·æœ‰åˆ›æ–°æ€§ã€çªç ´æ€§ï¼Œæˆ–æ¥è‡ªçŸ¥åæœºæ„ï¼ˆå¦‚ OpenAI, Google, Meta, DeepMind, æ–¯å¦ç¦ç­‰ï¼‰çš„è®ºæ–‡ã€‚
    2. å¦‚æœè®ºæ–‡å±äºæ™®é€šçš„å¢é‡ç ”ç©¶ã€ç»¼è¿°æˆ–è´¨é‡å¹³å¹³ï¼Œè¯·ä»…å›å¤â€œSKIPâ€å››ä¸ªå­—æ¯ã€‚

    æ€»ç»“æ ¼å¼ï¼š
    0. ã€åŸæ–‡æ ‡é¢˜ä¸æ‘˜è¦æ¦‚æ‹¬ã€‘
    1. ã€æ ¸å¿ƒè´¡çŒ®ã€‘
    2. ã€å¤§ç™½è¯å¯å‘ã€‘
    3. ã€åè¯è§£é‡Šã€‘
    æ³¨æ„ï¼š0-2ä¸è¶…è¿‡400å­—ã€‚
    """

    # --- ç»Ÿä¸€è¿”å›æ ¼å¼ï¼š(å†…å®¹, æ¨¡å‹å) ---
    try:
        print(f"ğŸ¤– Gemini æ­£åœ¨è¯„ä¼°: {title[:30]}...")
        genai.configure(api_key=GEMINI_KEY)
        model = genai.GenerativeModel('models/gemma-3-27b-it')
        response = model.generate_content(prompt)
        return response.text.strip(), "Gemma-3-27b"
    except Exception as e:
        print(f"âš ï¸ Gemini æŠ¥é”™: {e}ï¼Œå°è¯•åˆ‡æ¢ DeepSeek...")
        
        if DEEPSEEK_KEY:
            try:
                client = OpenAI(api_key=DEEPSEEK_KEY, base_url="https://api.deepseek.com")
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡è¯„å®¡åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.choices[0].message.content.strip(), "DeepSeek-V3"
            except Exception as ds_e:
                print(f"âŒ å…¨éƒ¨ AI å¤±è´¥: {ds_e}")
                return "ERROR", "None"
        return "ERROR", "None"

def fetch_and_summarize():
    if not GEMINI_KEY:
        print("é”™è¯¯: è¯·å…ˆé…ç½®ç¯å¢ƒå˜é‡")
        return

    # 1. è·å–æ•°æ®
    
    encoded_topic = urllib.parse.quote(TOPIC)
    api_url = f"http://export.arxiv.org/api/query?search_query={encoded_topic}&max_results=15&sortBy=submittedDate&sortOrder=descending"
    print(f"æ­£åœ¨æŠ“å– {TOPIC} çš„æœ€æ–°å†…å®¹...")
    feed = feedparser.parse(api_url)
    
    if not feed.entries:
        print("æš‚æ—¶æ²¡æŠ“åˆ°æ•°æ®ã€‚")
        return

    # 2. ç¡®å®šæ—¶é—´
    now_bj = datetime.datetime.utcnow() + datetime.timedelta(hours=8)
    report_type = "ğŸŒ… AI è®ºæ–‡æ—©æŠ¥" if 6 <= now_bj.hour <= 15 else "ğŸŒ™ AI è®ºæ–‡æ™šæŠ¥"

    # 3. å¤„ç†è®ºæ–‡
    read_papers = load_read_papers()
    new_paper_count = 0 
    processed_count = 0

    print("-" * 30)
    for entry in feed.entries:
        if entry.id in read_papers:
            continue 
        
        processed_count += 1
        if processed_count > 1:
            print(f"â³ å†·å´ 10 ç§’...")
            time.sleep(10)

        # ã€æ ¸å¿ƒä¿®æ­£ã€‘ï¼šä¸€æ¬¡è°ƒç”¨ï¼ŒåŒæ—¶è·å–ç»“æœå’Œæ¨¡å‹å
        result, model_name = get_ai_summary(entry.title, entry.summary.replace('\n', ' '))
        
        if result == "SKIP":
            print(f"ğŸƒ è·³è¿‡ä½ç›¸å…³æ€§è®ºæ–‡: {entry.title[:30]}...")
            save_read_paper(entry.id)
            continue
        
        if result == "ERROR":
            print(f"âŒ å¤„ç†å¤±è´¥ï¼Œè·³è¿‡")
            continue

        # 4. æ¨é€
        new_paper_count += 1
        # ä¿®æ­£å˜é‡å rfooter -> footer
        footer = f"\n\n---\n> ğŸ¤– **AI ç½²å**ï¼šæœ¬æ–‡ç”± {model_name} è‡ªåŠ¨æ€»ç»“ç”Ÿæˆ"
        report_content = f"### {report_type} (#{new_paper_count})\n\n{result}{footer}\n\nğŸ”— [æŸ¥çœ‹ ArXiv åŸæ–‡]({entry.link})"
        
        send_to_wechat(report_content)
        save_read_paper(entry.id)
        print(f"âœ… å·²æ¨é€: {entry.title[:30]}")
        print("-" * 30)

    if new_paper_count == 0:
        print(f"â˜• {report_type}: æš‚æ—¶æ²¡æœ‰ç¬¦åˆç­›é€‰æ ‡å‡†çš„æ–°è®ºæ–‡ã€‚")

if __name__ == "__main__":
    fetch_and_summarize()
